{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4v9_Zlk4_yD"
      },
      "source": [
        "Mike and Spencer\n",
        "\n",
        "### Beyond the Emergency Department: Predictive Analytics for 6-Month Patient Admission in Harmony Healthcare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGpq5Vxm5Lfy"
      },
      "source": [
        "### Problem Statement\n",
        "\n",
        "Our goal is to build a predictive model that identifies whether a patient will experience at least one ED admission within the next six months. Based on instructor feedback, we removed columns directly tied to emergency department data, forcing the model to learn broader risk signals rather than simply reproducing ED activity. This notebook documents data cleaning, exploratory visualization, and an initial predictive model using forward feature selection with logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "70011e73",
        "outputId": "cdd05bb8-8cee-45be-eba8-ba0fa2c44349"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import pandas as pd\n",
        "# drive.mount('/content/drive')\n",
        "# Then, update the path in pd.read_excel, for example:\n",
        "# df = pd.read_excel('/content/drive/MyDrive/Data Science CSC 108/HarmonyHealthcareOneWeek_9_2025.xlsx')\n",
        "\n",
        "df = pd.read_excel('HarmonyHealthcareOneWeek_9_2025.xlsx')\n",
        "\n",
        "# We want to keep the 'ED Episode Admit Last-6-Mths' column and remove all other columns with ED in the name\n",
        "ed_columns = [col for col in df.columns if 'ED' in col and col != 'ED Episode Admit Last-6-Mths']\n",
        "df = df.drop(columns=ed_columns)\n",
        "\n",
        "# Assuming NaN in 'ED Episode Admit Last-6-Mths' means no admission, fill with 0 early\n",
        "target = 'ED Episode Admit Last-6-Mths'\n",
        "df[target] = df[target].fillna(0)\n",
        "\n",
        "# Now we can remove any columns that are just empty to shrink the data further\n",
        "df = df.dropna(axis=1, how='all')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hJYRjKzkUpf",
        "outputId": "7bc71e6d-15fe-49ad-e5b1-63e1abb99bf0"
      },
      "outputs": [],
      "source": [
        "target = 'ED Episode Admit Last-6-Mths' # Store our target var so we can use it later\n",
        "\n",
        "# Check target distribution\n",
        "print(df[target].value_counts())\n",
        "print(df[target].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Check missing percentages\n",
        "missing_pct = (df.isna().mean() * 100).sort_values(ascending=False)\n",
        "print(missing_pct[missing_pct > 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "K3WxPQU0lLYW",
        "outputId": "ccff50ad-a134-415b-d2b8-ed27cd0322e1"
      },
      "outputs": [],
      "source": [
        "# Drop columns with >70% missing, except target\n",
        "columns_to_drop = [c for c in df.columns if c != target and df[c].isna().mean() * 100 > 70]\n",
        "print(\"Number of columns to drop:\", len(columns_to_drop))\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8LR2UkBogpQ",
        "outputId": "5c26da64-f46e-43c8-f4a3-8f5ab2553aff"
      },
      "outputs": [],
      "source": [
        "# Impute missing values + encode categoricals\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "target_col = 'ED Episode Admit Last-6-Mths' # Define target here too for consistency\n",
        "\n",
        "# Separate numerical + categorical\n",
        "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "# Exclude the target column from numeric imputation\n",
        "if target_col in numeric_cols:\n",
        "    numeric_cols.remove(target_col)\n",
        "\n",
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Impute numerics with median\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Impute categoricals with most frequent\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
        "\n",
        "# One-hot encode categoricals\n",
        "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DjdRp0VOooup",
        "outputId": "4ca2e397-3f5b-44e8-ce37-269b960ceb96"
      },
      "outputs": [],
      "source": [
        "# Visualizations!\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Target distribution\n",
        "df[target].value_counts().plot(kind='bar')\n",
        "plt.title(\"Target Distribution: 6-Month Admission\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Histogram of top numeric features\n",
        "# df[numeric_cols].hist(figsize=(12,10))\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# 3. Correlation heatmap\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df[numeric_cols].corr(), cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hhnzl6b5qqkR"
      },
      "outputs": [],
      "source": [
        "# Convert target to binary (admitted = 1 if >0)\n",
        "y = (df[target] > 0).astype(int)\n",
        "X = df.drop(columns=[target])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkA2dCpM6PGX"
      },
      "outputs": [],
      "source": [
        "# Clean and prepare data\n",
        "\n",
        "# Make a copy of the dataframe to avoid modifying the original `df` from the previous cell.\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Target column name\n",
        "target_col_name = \"ED Episode Admit Last-6-Mths\"\n",
        "\n",
        "# Isolate the target variable BEFORE any potentially destructive transformations.\n",
        "# Apply fillna(0) and >0 to ensure binary classification, and then convert to int.\n",
        "y = (df_processed[target_col_name].fillna(0) > 0).astype(int)\n",
        "\n",
        "# Drop the target column from the feature set X\n",
        "X = df_processed.drop(columns=[target_col_name])\n",
        "\n",
        "# Identify columns that are actual datetime objects in X.\n",
        "# These are the columns from `df.info()` with dtypes `datetime64[ns]` from the previous step.\n",
        "datetime_cols_in_X = X.select_dtypes(include=['datetime64[ns]']).columns.tolist()\n",
        "\n",
        "# Convert identified datetime columns in X to numeric days since epoch.\n",
        "# This avoids incorrectly converting other numeric columns.\n",
        "for col in datetime_cols_in_X:\n",
        "    X[col] = (X[col] - pd.Timestamp(\"1970-01-01\")).dt.days\n",
        "\n",
        "# Replace remaining missing numerical values in X with column medians.\n",
        "# This imputation step is for the features (X) after date conversion.\n",
        "X = X.fillna(X.median(numeric_only=True))\n",
        "\n",
        "# Train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize numerical data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhRQP95M6UZp"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression + evaluation function\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def evaluate(feature_list):\n",
        "    idxs = [X.columns.get_loc(f) for f in feature_list]\n",
        "\n",
        "    model = LogisticRegression(\n",
        "        max_iter=500,\n",
        "        class_weight=\"balanced\"\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_scaled[:, idxs], y_train)\n",
        "\n",
        "    preds = model.predict_proba(X_test_scaled[:, idxs])[:, 1]\n",
        "\n",
        "    return roc_auc_score(y_test, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqCV2suP6xhi",
        "outputId": "ca4cd541-3b79-4c7e-b681-3ce59c1f4eb4"
      },
      "outputs": [],
      "source": [
        "# Greedy forward feature selection\n",
        "\n",
        "# Initialize lists: 'remaining' holds features not yet selected, 'selected' holds chosen features,\n",
        "# and 'scores' stores the AUC for the selected feature set at each step.\n",
        "remaining = list(X.columns)\n",
        "selected = []\n",
        "scores = []\n",
        "\n",
        "# Perform 10 steps of forward feature selection.\n",
        "# In each step, we find the single best feature to add to our 'selected' set.\n",
        "for step in range(10):\n",
        "    best_feature = None\n",
        "    best_auc = -1 # Initialize with a low AUC score to ensure the first valid AUC is higher\n",
        "\n",
        "    # Iterate through all features not yet selected to find the one that maximizes AUC when added.\n",
        "    for feat in remaining:\n",
        "        # Create a temporary list of features that includes currently selected features plus one 'candidate' feature.\n",
        "        try_features = selected + [feat]\n",
        "        # Evaluate the performance (AUC) of the model using this candidate set of features.\n",
        "        auc = evaluate(try_features)\n",
        "\n",
        "        # If this candidate set yields a better AUC than the current best, update best_auc and best_feature.\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_feature = feat\n",
        "\n",
        "    # Add the best performing feature from this step to the 'selected' list.\n",
        "    selected.append(best_feature)\n",
        "    # Record the AUC achieved with this new set of selected features.\n",
        "    scores.append(best_auc)\n",
        "    # Remove the selected feature from the 'remaining' list so it's not considered again.\n",
        "    remaining.remove(best_feature)\n",
        "\n",
        "    # Print the result for the current step.\n",
        "    # print(f\"Step {step+1}: Selected {best_feature} — AUC {best_auc:.4f}\")\n",
        "\n",
        "# After all steps are complete, print the final list of top 10 selected features and their corresponding AUC scores.\n",
        "print(\"\\nTop 10 selected features:\")\n",
        "for i, (feat, auc) in enumerate(zip(selected, scores), 1):\n",
        "    print(f\"{i}. {feat} — AUC {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "_q9a1wf8nS7W",
        "outputId": "fb28cc31-fde4-4e23-c12a-7dffca61f3c3"
      },
      "outputs": [],
      "source": [
        "# Are the top 10 selected features correlated to each other?\n",
        "selected_df = df[selected + [target]]\n",
        "corr = selected_df.corr()\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr, annot=False, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation among Greedy Selected Top 10 Features\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrS2nNmjJZEh",
        "outputId": "adf94b8c-54d5-476a-d4e6-7caad0155343"
      },
      "outputs": [],
      "source": [
        "# 5 fold Cross-Validation\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Use only the greedy-selected top 10 features\n",
        "X_selected = X[selected]\n",
        "\n",
        "# Build pipeline to avoid data leakage\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"logreg\", LogisticRegression(\n",
        "        max_iter=500,\n",
        "        class_weight=\"balanced\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 5-fold stratified CV\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    pipeline,\n",
        "    X_selected,\n",
        "    y,\n",
        "    cv=cv,\n",
        "    scoring=\"roc_auc\"\n",
        ")\n",
        "\n",
        "print(\"5-fold CV AUC scores:\", cv_scores)\n",
        "print(\"Mean CV AUC:\", np.mean(cv_scores))\n",
        "print(\"Std CV AUC:\", np.std(cv_scores))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "\n",
        "def lasso_classification_comparison(X, y, greedy_selected_features, greedy_auc):\n",
        "    \n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "    \n",
        "    # Use LogisticRegressionCV with L1 penalty (Lasso for classification)\n",
        "    # C is inverse of regularization strength (smaller C = more regularization)\n",
        "    Cs = np.logspace(-4, 2, 20)  # Test range of regularization strengths\n",
        "    \n",
        "    # add verbose=1 to see progress\n",
        "    lasso_logistic = LogisticRegressionCV(\n",
        "        Cs=Cs,\n",
        "        cv=5,\n",
        "        penalty='l1',\n",
        "        solver='saga',  # Required for L1 penalty\n",
        "        max_iter=50000,\n",
        "        class_weight='balanced',  # Handle class imbalance like your greedy method\n",
        "        scoring='roc_auc',  # Use AUC like your greedy method\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    print(\"\\nFitting L1-regularized Logistic Regression...\")\n",
        "    lasso_logistic.fit(X_scaled, y)\n",
        "    \n",
        "    print(f\"Optimal C (inverse regularization): {lasso_logistic.C_[0]:.6f}\")\n",
        "    print(f\"  (Smaller C = more regularization, like larger alpha in Lasso)\")\n",
        "    \n",
        "    # Get coefficients\n",
        "    coefficients = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'coefficient': lasso_logistic.coef_[0]\n",
        "    })\n",
        "    \n",
        "    # Select non-zero features\n",
        "    selected_features = coefficients[coefficients['coefficient'] != 0].copy()\n",
        "    selected_features = selected_features.sort_values(\n",
        "        'coefficient', \n",
        "        key=abs, \n",
        "        ascending=False\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nNumber of features selected: {len(selected_features)}/{len(X.columns)}\")\n",
        "    print(f\"Feature reduction: {(1 - len(selected_features)/len(X.columns))*100:.1f}%\")\n",
        "    \n",
        "    print(\"\\nTop 20 selected features and coefficients:\")\n",
        "    print(selected_features.head(20).to_string(index=False))\n",
        "    \n",
        "    if len(selected_features) > 20:\n",
        "        print(f\"\\n... and {len(selected_features) - 20} more features\")\n",
        "    \n",
        "    # Calculate performance metrics using CLASSIFICATION metrics\n",
        "    y_pred_proba = lasso_logistic.predict_proba(X_scaled)[:, 1]\n",
        "    y_pred = lasso_logistic.predict(X_scaled)\n",
        "    \n",
        "    train_auc = roc_auc_score(y, y_pred_proba)\n",
        "    train_acc = accuracy_score(y, y_pred)\n",
        "    \n",
        "    print(f\"\\nTRAIN Performance:\")\n",
        "    print(f\"  AUC: {train_auc:.4f}\")\n",
        "    print(f\"  Accuracy: {train_acc:.4f}\")\n",
        "    \n",
        "    # Cross-validation with proper classification scoring\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_auc_scores = cross_val_score(\n",
        "        lasso_logistic, X_scaled, y, cv=cv, scoring='roc_auc'\n",
        "    )\n",
        "    cv_acc_scores = cross_val_score(\n",
        "        lasso_logistic, X_scaled, y, cv=cv, scoring='accuracy'\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nCROSS-VALIDATION Performance (5-fold):\")\n",
        "    print(f\"  AUC: {cv_auc_scores.mean():.4f} ± {cv_auc_scores.std():.4f}\")\n",
        "    print(f\"  Accuracy: {cv_acc_scores.mean():.4f} ± {cv_acc_scores.std():.4f}\")\n",
        "    \n",
        "    # Comparison with Greedy Forward Selection\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARISON: L1 Logistic Regression vs Greedy Forward Selection\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(f\"\\nNumber of features:\")\n",
        "    print(f\"  Greedy Forward Selection: {len(greedy_selected_features)}\")\n",
        "    print(f\"  L1 Logistic (Lasso):      {len(selected_features)}\")\n",
        "    \n",
        "    print(f\"\\nCross-Validation AUC:\")\n",
        "    print(f\"  Greedy Forward Selection: {greedy_auc:.4f}\")\n",
        "    print(f\"  L1 Logistic (Lasso):      {cv_auc_scores.mean():.4f} ± {cv_auc_scores.std():.4f}\")\n",
        "    \n",
        "    # Feature overlap analysis\n",
        "    lasso_set = set(selected_features['feature'].tolist())\n",
        "    greedy_set = set(greedy_selected_features)\n",
        "    \n",
        "    common = lasso_set & greedy_set\n",
        "    only_lasso = lasso_set - greedy_set\n",
        "    only_greedy = greedy_set - lasso_set\n",
        "    \n",
        "    print(f\"\\nFeature Overlap:\")\n",
        "    print(f\"  Common features: {len(common)}/{len(greedy_set)}\")\n",
        "    if common:\n",
        "        print(f\"    {sorted(list(common)[:10])}\")\n",
        "        if len(common) > 10:\n",
        "            print(f\"    ... and {len(common) - 10} more\")\n",
        "    \n",
        "    print(f\"\\n  Only in L1 Logistic: {len(only_lasso)}\")\n",
        "    if only_lasso and len(only_lasso) <= 10:\n",
        "        print(f\"    {sorted(list(only_lasso))}\")\n",
        "    elif only_lasso:\n",
        "        print(f\"    {sorted(list(only_lasso))[:10]}\")\n",
        "        print(f\"    ... and {len(only_lasso) - 10} more\")\n",
        "    \n",
        "    print(f\"\\n  Only in Greedy: {len(only_greedy)}\")\n",
        "    if only_greedy:\n",
        "        print(f\"    {sorted(list(only_greedy))}\")\n",
        "    \n",
        "    # Interpretation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"INTERPRETATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if cv_auc_scores.mean() > 0.7:\n",
        "        print(\"✓ Good predictive performance (AUC > 0.7)\")\n",
        "    elif cv_auc_scores.mean() > 0.6:\n",
        "        print(\"○ Moderate predictive performance (AUC 0.6-0.7)\")\n",
        "    else:\n",
        "        print(\"✗ Weak predictive performance (AUC < 0.6)\")\n",
        "        print(\"  Consider: feature engineering, interaction terms, or different algorithms\")\n",
        "    \n",
        "    if len(selected_features) < len(X.columns) * 0.1:\n",
        "        print(f\"✓ Strong regularization: using only {len(selected_features)} features\")\n",
        "        print(\"  This suggests most features don't contribute to prediction\")\n",
        "    \n",
        "    if abs(cv_auc_scores.mean() - greedy_auc) < 0.02:\n",
        "        print(\"≈ Both methods achieve similar performance\")\n",
        "        if len(selected_features) < len(greedy_selected_features):\n",
        "            print(\"  → L1 Logistic is more parsimonious (fewer features, similar performance)\")\n",
        "        else:\n",
        "            print(\"  → Greedy is more parsimonious (fewer features, similar performance)\")\n",
        "    \n",
        "    return {\n",
        "        'model': lasso_logistic,\n",
        "        'scaler': scaler,\n",
        "        'selected_features': selected_features,\n",
        "        'feature_names': selected_features['feature'].tolist(),\n",
        "        'train_auc': train_auc,\n",
        "        'cv_auc_mean': cv_auc_scores.mean(),\n",
        "        'cv_auc_std': cv_auc_scores.std(),\n",
        "        'cv_auc_scores': cv_auc_scores,\n",
        "        'optimal_C': lasso_logistic.C_[0]\n",
        "    }\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# After the greedy forward selection completes you have:\n",
        "# - selected: list of top 10 features from greedy\n",
        "# - scores: list of AUC scores from greedy\n",
        "# - X: your full feature matrix\n",
        "# - y: your binary target\n",
        "\"\"\"\n",
        "greedy_best_auc = scores[-1]  # Last score is with all 10 features\n",
        "\n",
        "lasso_results = lasso_classification_comparison(\n",
        "    X=X,\n",
        "    y=y,\n",
        "    greedy_selected_features=selected,\n",
        "    greedy_auc=greedy_best_auc\n",
        ")\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ZwML3FOx6e"
      },
      "source": [
        "### Discussion of Early Results and Moving Forward\n",
        "\n",
        "* The greedy selection approach identified a ranked set of features contributing incremental predictive value.\n",
        "* The AUC values supply an interpretable measure of classification.\n",
        "* Further improvements planned include:\n",
        "    1. Cross validate the model with testing data\n",
        "    2. Plot the AUC for more than 10 features\n",
        "    3. Check correlation of selected features to make sure we are not selecting highly correlated features\n",
        "    4. Attempt to compare to Lasso if enough time\n",
        "\n",
        "The project was a collaborative effort between Mike and Spencer with both of us working on the data cleaning step and Mike taking charge of the algorithm and Spencer doing the write up and github steps.\n",
        "\n",
        "\n",
        "### Later Results and Finalizations (final Jupyter notebook)\n",
        "\n",
        "After the initial notebook, we accomplished most of our future plans. Spencer moved forward with implementing the lasso algorithm to compare to the previous team working on this dataset per professor feedback, and Mike implemented the 5-fold cross validation and added a heatmap to check if the top 10 selected features from the greedy algorithm are correlated to each other.\n",
        "* Successful improvements we managed to do after the initial notebook:\n",
        "    1. Cross validate the model with 5-fold CV\n",
        "    2. Check correlation of selected features using heatmap\n",
        "    3. Compare to Lasso algorithm\n",
        "\n",
        "The project can be shared with future students.\n",
        "\n",
        "Github link: https://github.com/99x5zbrvgj-droid/CSC108HHCFinal"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
